from utils import search_file, get_api_name, get_dictionary, get_tb_info, get_external, compute_precision_recall, is_csv_empty, clear_name
from reconstruct_data import remove_digits, compress_ddl
import os
import json
import csv
from tqdm import tqdm
from chat import GPTChat
from concurrent.futures import ThreadPoolExecutor, as_completed
import argparse
import sys
import re
import numpy as np

# Windows í™˜ê²½ì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œ 32ë¹„íŠ¸ ì‹œìŠ¤í…œ ìµœëŒ€ê°’ìœ¼ë¡œ ì„¤ì •
try:
    csv.field_size_limit(sys.maxsize)
except OverflowError:
    csv.field_size_limit(2147483647)  # 32ë¹„íŠ¸ ì‹œìŠ¤í…œ ìµœëŒ€ê°’
    
# THRESHOLD = 200000
THRESHOLD = 50000  # 50KBë¡œ ë‚®ì¶¤ (FNF íƒœìŠ¤í¬ ê³ ë ¤)
DEPS_DEV_V1 = ["sf_bq016", "sf_bq062", "sf_bq063", "sf_bq028"]

def reduce_columns(sql: str, subset_columns: set[str]) -> str:

    table_match = re.search(r'create\s+(?:or\s+replace\s+)?table\s+`?([^\s(]+)`?', sql, re.IGNORECASE)
    assert table_match, sql
    table_name = table_match.group(1)

    # ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ ì»¬ëŸ¼ ë¸”ë¡ ì¶”ì¶œ (ì •ê·œì‹ë³´ë‹¤ ì•ˆì •ì )
    start_idx = sql.find('(')
    if start_idx == -1:
        raise ValueError("Cannot find opening parenthesis.")
    
    # ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ ë ì°¾ê¸°
    paren_count = 0
    end_idx = start_idx
    
    for i, char in enumerate(sql[start_idx:], start_idx):
        if char == '(':
            paren_count += 1
        elif char == ')':
            paren_count -= 1
            if paren_count == 0:
                end_idx = i
                break
    
    if paren_count != 0:
        raise ValueError("Cannot find matching closing parenthesis.")
    
    columns_block = sql[start_idx+1:end_idx]

    lines = columns_block.splitlines()
    filtered_lines = []
    for line in lines:
        line = line.strip().rstrip(',')
        if not line:
            continue
        # primary key ê°™ì€ ì œì•½ì¡°ê±´ì€ ìŠ¤í‚µ
        if line.lower().startswith('primary key') or line.lower().startswith('foreign key') or line.lower().startswith('unique'):
            continue
        parts = line.split()
        if len(parts) < 2:
            continue
        col_name = parts[0].strip('`",')
        if col_name in subset_columns:
            filtered_lines.append(f"  {line},")

    if filtered_lines:
        filtered_lines[-1] = filtered_lines[-1].rstrip(',')

    new_sql = f'CREATE TABLE {table_name} (\n' + '\n'.join(filtered_lines) + '\n);'
    return new_sql


def reduce_ddl(example_path, dictionaries, linked_json, reduce_col=False):
    # ìŠ¤í‚¤ë§ˆ ë§í‚¹ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ í…Œì´ë¸” ì œê±°
    # ì»¬ëŸ¼ ë ˆë²¨ í•„í„°ë§ìœ¼ë¡œ ì¶”ê°€ ì••ì¶•
    
    print(f"ğŸ“‹ DDL ì¶•ì†Œ ëŒ€ìƒ: {len(dictionaries)}ê°œ ì˜ˆì œ")
    print(f"ğŸ“„ ìŠ¤í‚¤ë§ˆ ë§í‚¹ ê²°ê³¼ íŒŒì¼: {linked_json}")
    print(f"ğŸ¯ ì„ê³„ê°’: {THRESHOLD:,} bytes ì´ìƒì¸ í”„ë¡¬í”„íŠ¸ë§Œ ì²˜ë¦¬")
    print()
    
    processed_count = 0
    skipped_count = 0
    
    for eg_id in tqdm(dictionaries, desc="ğŸ“‚ DDL ì¶•ì†Œ ì²˜ë¦¬"):
        api = get_api_name(eg_id)
 
        ddl_paths = search_file(os.path.join(example_path, eg_id), "DDL.csv")

        # 1. í”„ë¡¬í”„íŠ¸ í¬ê¸° í™•ì¸ (í”„ë¡¬í”„íŠ¸ íŒŒì¼ì´ 200KB ì´ìƒì´ê±°ë‚˜, ê°œë°œ ë°ì´í„°ì…‹ì¸ ê²½ìš° ê±´ë„ˆëœ€)
        prompt_size = os.path.getsize(os.path.join(example_path, eg_id, "prompts.txt"))
        if prompt_size < THRESHOLD or eg_id in DEPS_DEV_V1:
            skipped_count += 1
            continue

        # 2. ìŠ¤í‚¤ë§ˆ ë§í‚¹ ê²°ê³¼ ë¡œë“œ
        with open(linked_json, encoding='utf-8') as f:
            sl = json.load(f)

        # 3. ìŠ¤í‚¤ë§ˆ ë§í‚¹ ê²°ê³¼ë¥¼ í†µí•´, íŒŒì‹±í•˜ì—¬ í…Œì´ë¸” ì´ë¦„ ì¶”ì¶œ
        table_names = []
        columns = {}
        for ex_id, tbs in sl.items():
            if ex_id == eg_id:
                for tb in tbs:
                    if "answer" in tb:
                        if tb["answer"] == "Y": # ê´€ë ¨ ìˆë‹¤ê³  íŒë‹¨ëœ í…Œì´ë¸”ë§Œ
                            table_names.append(tb["table name"])
                            columns[tb["table name"]] = tb['columns']
                    else:
                        raise NotImplementedError
                        print(tb)
                        table_names.append(tb)

        if not table_names:
            print(f"      âš ï¸  {eg_id}: ê´€ë ¨ í…Œì´ë¸” ì—†ìŒ, ìŠ¤í‚µ")
            skipped_count += 1
            continue
        
        processed_count += 1
        table_names_no_digit = [remove_digits(i) for i in table_names]

        temp_file_paths = []
        # 4. DDL.csv â†’ DDL_sl.csv í•„í„°ë§
        for ddl_path in ddl_paths:
            temp_file = ddl_path.replace("DDL.csv", "DDL_sl.csv")
            temp_file_paths.append(temp_file)
            with open(ddl_path, "r", newline="", encoding="utf-8", errors="ignore") as infile, \
                open(temp_file, "w", newline="", encoding="utf-8", errors="ignore") as outfile:
                
                reader = csv.reader(infile)
                writer = csv.writer(outfile)

                header = next(reader)
                writer.writerow(header)
                row_count = 0
                row_count_rm = 0
                total_count = 0
                row_list_all = []
                row_list = []
                for row in reader:
                    assert row[-1].upper().startswith("CREATE"), row
                    if "." in row[0]:
                        row[0] = row[0].split(".")[-1]

                    json_pth = ddl_path.replace("DDL.csv", row[0].strip()+".json")
                    if os.path.exists(json_pth):
                        with open(json_pth, encoding="utf-8") as f:
                            table_fullname = json.load(f)["table_fullname"]
                    else:
                        print(f"{ex_id}: {json_pth} doesn't exist")
                        continue
                    
                    if any(remove_digits(table_fullname) in item for item in table_names_no_digit):
                        row_count_rm += 1
                        row_list_all.append(row)

                    if any(table_fullname == item for item in table_names):

                        row_count += 1

                        if reduce_col:
                            assert table_fullname in columns, print(table_names, table_fullname)
                            row[-1] = reduce_columns(row[-1], columns[table_fullname])
                            # print("After", row)
                        row_list.append(row)
                    total_count += 1
                # ê²°ê³¼ í†µê³„ ì¶œë ¥ ë° íŒŒì¼ ì‘ì„± ê²°ì •
                if 0 < row_count < 10 or row_count_rm > 1000 or reduce_col:
                    writer.writerows(row_list)
                    result_type = "ì •í™•í•œ ë§¤ì¹­"
                elif row_count_rm:
                    writer.writerows(row_list_all)
                    result_type = "ìˆ«ì ì œê±° ë§¤ì¹­"
                else:
                    result_type = "ë§¤ì¹­ ì—†ìŒ"
                
        # ë¹ˆ DDL íŒŒì¼ ì •ë¦¬
        if all(is_csv_empty(i) for i in temp_file_paths):
            for i in temp_file_paths:
                os.remove(i)
    print()
    print(f"ğŸ“Š DDL ì¶•ì†Œ ì™„ë£Œ í†µê³„:")
    print(f"   - ì²˜ë¦¬ëœ ì˜ˆì œ: {processed_count}ê°œ")
    print(f"   - ìŠ¤í‚µëœ ì˜ˆì œ: {skipped_count}ê°œ")
    print(f"   - ì´ ì˜ˆì œ: {len(dictionaries)}ê°œ")
    print()
    
    print("ğŸ”„ ì¶•ì†Œëœ DDLì„ ë°”íƒ•ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì¬ìƒì„± ì¤‘...")
    compress_ddl(example_path, add_description=True, add_sample_rows=False, rm_digits=True, schema_linked=True, clear_long_eg_des=True, reduce_col=reduce_col)
    # compress_ddl(example_path, add_description=True, add_sample_rows=True, rm_digits=True, schema_linked=True, clear_long_eg_des=True, reduce_col=reduce_col)
    print("âœ… í”„ë¡¬í”„íŠ¸ ì¬ìƒì„± ì™„ë£Œ")

ask_prompt = """
í…Œì´ë¸” ìˆ˜ì¤€ ìŠ¤í‚¤ë§ˆ ë§í‚¹ì„ ìˆ˜í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ìˆëŠ” í…Œì´ë¸”ê³¼ ì‘ì—…ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ë‹¨ê³„ë³„ë¡œ ìƒê°í•˜ì—¬ ì´ í…Œì´ë¸”ì´ ì‘ì—…ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
Y/Nìœ¼ë¡œë§Œ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹µì´ Yì¸ ê²½ìš°, ê´€ë ¨ ìˆë‹¤ê³  ìƒê°í•˜ëŠ” ì»¬ëŸ¼ë“¤ì„ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ json ì½”ë“œ ë¸”ë¡ìœ¼ë¡œë§Œ ë‹µí•´ì£¼ì„¸ìš”:
```json
{{
    "think": "ê²°ì •í•˜ê¸° ìœ„í•´ ë‹¨ê³„ë³„ë¡œ ìƒê°",
    "answer": "Y ë˜ëŠ” Në§Œ",
    "columns": [col_name1, col_name2]
}}
```

í…Œì´ë¸” ì •ë³´: {0}
ì‘ì—…: {1}
{2}
"""

def ask_model_sl(example_path, json_save_pth):
    """
    Schema linkingì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
    Args:
        example_path (str): ì˜ˆì œ í´ë” ê²½ë¡œ
        json_save_pth (str): ìŠ¤í‚¤ë§ˆ ë§í‚¹ ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ

    Returns:
        linked_dic (dict): ìŠ¤í‚¤ë§ˆ ë§í‚¹ ê²°ê³¼
    """
    linked_dic = {}

    def process_example(ex_id):
        if ex_id.startswith("local"): # SQLite ê¸°ë°˜ local ì˜ˆì œëŠ” ìŠ¤í‚¤ë§ˆ ë§í‚¹ì—ì„œ ì œì™¸ (Local ì—ì œëŠ” ì´ë¯¸ ì‘ì€ ê·œëª¨)
            return None, None
        
        tb_info_pth = search_file(os.path.join(example_path, ex_id), "prompts.txt") # í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
        assert len(tb_info_pth) == 1
        with open(tb_info_pth[0], encoding="utf-8") as f:
            tb_info = f.read()

        task = task_dict[ex_id]
        chat_session = GPTChat(azure=False, model="gpt-4o", temperature=0)
        result = ask_model_sl_(tb_info, task, chat_session)
        return ex_id, result

    linked_dic = {}
    print("Doing table-level schema linking")

    # 32ê°œì˜ ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=32) as executor:
        futures = [executor.submit(process_example, ex_id) for ex_id in dictionaries]

        processed_count = 0
        for future in tqdm(as_completed(futures), total=len(futures), desc="ğŸ”— í…Œì´ë¸” ê´€ë ¨ì„± ë¶„ì„"):
            ex_id, result = future.result() # process_exampleì˜ ê²°ê³¼ ë°˜í™˜
            if ex_id is not None: # ì˜ˆì™¸ ì²˜ë¦¬
                linked_dic[ex_id] = result
                processed_count += 1

        print(f"âœ… ìŠ¤í‚¤ë§ˆ ë§í‚¹ ì™„ë£Œ: {processed_count}ê°œ ì˜ˆì œ ì²˜ë¦¬")
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘: {json_save_pth}")
        
        with open(json_save_pth, "w", encoding="utf-8") as f:
            json.dump(linked_dic, f, indent=4, ensure_ascii=False)

def ask_model_sl_(tb_info, task, chat_session): # sl: schema linking
    """
    GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê° í…Œì´ë¸”ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ”ì§€ íŒë‹¨í•˜ê³ ,
    Y/N ë‹µë³€ê³¼ í•¨ê»˜ ê´€ë ¨ ì»¬ëŸ¼ ëª©ë¡ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    Args:
        tb_info (str): í…Œì´ë¸” ì •ë³´
        task (str): ì‘ì—…
        chat_session (GPTChat): GPT ëª¨ë¸ ì„¸ì…˜

    Returns:
        linked (list): ìŠ¤í‚¤ë§ˆ ë§í‚¹ ê²°ê³¼
    """
    tbs = get_tb_info(tb_info)
    external = get_external(tb_info)
    linked = []

    for tb in tbs:
        chat_session.init_messages()
        max_try = 3
        input = ask_prompt.format(tb, task, external)
        while max_try:
            response = chat_session.get_model_response(input, "json")
            if len(response) == 1:
                response = response[0]
                try:
                    data = json.loads(response)
                    assert data["answer"] in ["Y", "N"], 'data["answer"] should be in ["Y", "N"]'
                    # ì˜ì–´ì™€ í•œê¸€ í…Œì´ë¸”ëª… ëª¨ë‘ ì§€ì›
                    table_name_match = re.search(r'^Table full name:\s*(.+)$', tb, re.MULTILINE)
                    if not table_name_match:
                        table_name_match = re.search(r'^í…Œì´ë¸” ì „ì²´ëª…:\s*(.+)$', tb, re.MULTILINE)
                    data["table name"] = table_name_match.group(1)
                    break
                except Exception as e:
                    input = e+"Please generate again."
            max_try -= 1
        if max_try == 0:
            # ì˜ì–´ì™€ í•œê¸€ í…Œì´ë¸”ëª… ëª¨ë‘ ì§€ì›
            table_name_match = re.search(r'^Table full name:\s*(.+)$', tb, re.MULTILINE)
            if not table_name_match:
                table_name_match = re.search(r'^í…Œì´ë¸” ì „ì²´ëª…:\s*(.+)$', tb, re.MULTILINE)
            if table_name_match:
                print("Failed", table_name_match.group(1))
            else:
                print("Failed to extract table name from:", tb[:100])
            continue
        # print(data)
        linked.append(data)

    return linked

def compute_metrics_sl(file_pth, db_path):
    print(f"ğŸ“Š ì„±ëŠ¥ í‰ê°€ íŒŒì¼ ë¡œë“œ: {file_pth}")
    with open(file_pth, encoding="utf-8") as f:
        data = json.load(f)
    
    count = 0
    precision_all = []
    recall_all = []
    perfect_recall_count = 0
    
    print(f"ğŸ“‹ í‰ê°€ ëŒ€ìƒ: {len(data)}ê°œ ì˜ˆì œ")
    
    for example, tbs in data.items():
        # ê³¨ë“œ í…Œì´ë¸” ì°¾ê¸°
        gold_table = None
        for ex in gold:
            if ex['instance_id'] == example:
                gold_table = set(ex["gold_tables"])
                break
        
        if gold_table is None:
            continue

        # ì„ê³„ê°’ ì´ìƒì¸ ì˜ˆì œë§Œ í‰ê°€
        if os.path.getsize(os.path.join(db_path, example, "prompts.txt")) > THRESHOLD:
            count += 1
            pred = []
            
            # ì˜ˆì¸¡ëœ í…Œì´ë¸” ì¶”ì¶œ
            for tb in tbs:
                if "answer" in tb:
                    if tb["answer"] == "Y":
                        pred.append(tb["table name"])
                else:
                    print(f"      âš ï¸  ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹: {tb}")
                    pred.append(tb)
            
            # Precision/Recall ê³„ì‚°
            precision, recall = compute_precision_recall(clear_name(pred), clear_name(gold_table))
            
            if precision != 0 and recall != 0:
                if recall == 1.0:
                    perfect_recall_count += 1
                else:
                    print(f"      ğŸ“‰ ë¶ˆì™„ì „í•œ Recall: {example} (P: {precision:.3f}, R: {recall:.3f})")
                
                precision_all.append(precision)
                recall_all.append(recall)
    
    # ìµœì¢… í†µê³„
    if precision_all and recall_all:
        mean_precision = np.mean(precision_all)
        mean_recall = np.mean(recall_all)
        imperfect_recall_count = np.sum(np.array(recall_all) < 1)
        
        print()
        print("ğŸ“Š ìŠ¤í‚¤ë§ˆ ë§í‚¹ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼:")
        print(f"   - í‰ê°€ëœ ì˜ˆì œ ìˆ˜: {count}ê°œ")
        print(f"   - í‰ê·  Precision: {mean_precision:.3f}")
        print(f"   - í‰ê·  Recall: {mean_recall:.3f}")
        print(f"   - ì™„ë²½í•œ Recall (1.0): {perfect_recall_count}ê°œ")
        print(f"   - ë¶ˆì™„ì „í•œ Recall (<1.0): {imperfect_recall_count}ê°œ")
    else:
        print("âš ï¸  í‰ê°€í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")  

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--task', type=str, default="lite")
    parser.add_argument('--db_path', type=str, default="examples_lite_full")
    parser.add_argument('--linked_json_pth', type=str, default=None)
    parser.add_argument('--reduce_col', action="store_true")
    parser.add_argument('--gold_tb_pth', type=str, default=None)
    args = parser.parse_args()

    print("ğŸ¯ Schema Linking ë° DDL ì¶•ì†Œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤")
    print(f"ğŸ“‚ ëŒ€ìƒ í´ë”: {args.db_path}")
    print(f"ğŸ·ï¸  íƒœìŠ¤í¬: {args.task}")
    print(f"ğŸ“„ ìŠ¤í‚¤ë§ˆ ë§í‚¹ ê²°ê³¼ íŒŒì¼: {args.linked_json_pth}")
    print(f"ğŸ”§ ì»¬ëŸ¼ ì¶•ì†Œ: {'âœ…' if args.reduce_col else 'âŒ'}")
    print(f"ğŸ† ê³¨ë“œ í…Œì´ë¸” íŒŒì¼: {args.gold_tb_pth if args.gold_tb_pth else 'âŒ ì—†ìŒ'}")
    print()

    print("=" * 60)
    print("ğŸ“‹ STEP 1: ë”•ì…”ë„ˆë¦¬ ë° íƒœìŠ¤í¬ ì •ë³´ ë¡œë“œ")
    print("=" * 60)
    dictionaries, task_dict = get_dictionary(args.db_path, args.task)
    print(f"âœ… ë”•ì…”ë„ˆë¦¬ ë¡œë“œ ì™„ë£Œ: {len(dictionaries)}ê°œ ì˜ˆì œ")
    print(f"âœ… íƒœìŠ¤í¬ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {len(task_dict)}ê°œ ì§ˆë¬¸")
    print()
    
    # ìŠ¤í‚¤ë§ˆ ë§í‚¹ ìˆ˜í–‰ (linked_json_pthê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œë§Œ) -- ë¹„ì‹¼ ì‘ì—…ì´ë¼, ì´ë¯¸ ìˆ˜í–‰ëœ ê²½ìš° ê±´ë„ˆëœ€
    if args.linked_json_pth is not None and not os.path.exists(args.linked_json_pth):
        print("=" * 60)
        print("ğŸ§  STEP 2: Schema Linking ìˆ˜í–‰")
        print("=" * 60)
        print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {args.linked_json_pth}")
        print("ğŸš€ GPT ëª¨ë¸ì„ ì‚¬ìš©í•œ ìŠ¤í‚¤ë§ˆ ë§í‚¹ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        print()
        
        gold = []  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì„¤ì •
        
        # ê³¨ë“œ í…Œì´ë¸” íŒŒì¼ì´ ì œê³µë˜ê³  ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ ë¡œë“œ
        print("ğŸ† ê³¨ë“œ í…Œì´ë¸” ë¡œë“œ ì‹œë„ ì¤‘...")
        if args.gold_tb_pth is not None and os.path.exists(args.gold_tb_pth):
            try:
                with open(args.gold_tb_pth, encoding="utf-8") as f:
                    gold = [json.loads(i) for i in f]
                print(f"âœ… ê³¨ë“œ í…Œì´ë¸” ë¡œë“œ ì™„ë£Œ: {len(gold)}ê°œ ì¸ìŠ¤í„´ìŠ¤")
            except Exception as e:
                print(f"âŒ ê³¨ë“œ í…Œì´ë¸” ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("âš ï¸  ê³¨ë“œ í…Œì´ë¸” ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
                gold = []
        else:
            if args.gold_tb_pth is None:
                print("â„¹ï¸  ê³¨ë“œ í…Œì´ë¸” ê²½ë¡œê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê³¨ë“œ í…Œì´ë¸” ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
            else:
                print(f"âŒ ê³¨ë“œ í…Œì´ë¸” íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {args.gold_tb_pth}")
                print("âš ï¸  ê³¨ë“œ í…Œì´ë¸” ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
        print()

        # ìŠ¤í‚¤ë§ˆ ë§í‚¹ ìˆ˜í–‰
        print("ğŸ”— í…Œì´ë¸”-íƒœìŠ¤í¬ ê´€ë ¨ì„± ë¶„ì„ ì‹œì‘...")
        ask_model_sl(args.db_path, args.linked_json_pth)
        print(f"âœ… ìŠ¤í‚¤ë§ˆ ë§í‚¹ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {args.linked_json_pth}")
        print()

        # ê³¨ë“œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ë©”íŠ¸ë¦­ ê³„ì‚°
        if gold:
            print("ğŸ“Š ì„±ëŠ¥ í‰ê°€ ì‹œì‘...")
            print("ğŸ† ê³¨ë“œ í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤í‚¤ë§ˆ ë§í‚¹ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.")
            compute_metrics_sl(args.linked_json_pth, args.db_path)
            print("âœ… ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ")
        else:
            print("â„¹ï¸  ê³¨ë“œ í…Œì´ë¸”ì´ ì—†ì–´ ì„±ëŠ¥ í‰ê°€ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
        print()
    else:
        print("=" * 60)
        print("â­ï¸  STEP 2: Schema Linking ìŠ¤í‚µ")
        print("=" * 60)
        if args.linked_json_pth is None:
            print("â„¹ï¸  ìŠ¤í‚¤ë§ˆ ë§í‚¹ ê²°ê³¼ íŒŒì¼ ê²½ë¡œê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            print(f"âœ… ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ ë§í‚¹ ê²°ê³¼ íŒŒì¼ ë°œê²¬: {args.linked_json_pth}")
            print("ğŸ’° ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ ê¸°ì¡´ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.")
        print()
    
    # DDL ì¶•ì†Œ ìˆ˜í–‰ (í•­ìƒ ì‹¤í–‰)
    print("=" * 60)
    print("âœ‚ï¸  STEP 3: DDL ì¶•ì†Œ ë° í”„ë¡¬í”„íŠ¸ ì¬ìƒì„±")
    print("=" * 60)
    print("ğŸ—„ï¸  ìŠ¤í‚¤ë§ˆ ë§í‚¹ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ í…Œì´ë¸”ì„ ì œê±°í•©ë‹ˆë‹¤...")
    print(f"ğŸ”§ ì»¬ëŸ¼ ë ˆë²¨ ì¶•ì†Œ: {'âœ… í™œì„±í™”' if args.reduce_col else 'âŒ ë¹„í™œì„±í™”'}")
    print()
    
    reduce_ddl(args.db_path, dictionaries, args.linked_json_pth, args.reduce_col)
    
    print("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ğŸ“ ì¶•ì†Œëœ DDL íŒŒì¼ë“¤: {args.db_path}/*/DDL_sl.csv")
    print(f"ğŸ“„ ì¬ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë“¤: {args.db_path}/*/prompts.txt")
    print("ğŸ”§ ë‹¤ìŒ ë‹¨ê³„: run.py ì‹¤í–‰")