import os
import pandas as pd
from tqdm import tqdm
import argparse
import shutil
import sqlite3
from utils import remove_digits, is_file, clear_description, clear_sample_rows, extract_column_names, extract_real_table_names, get_api_name, clear_name, remove_declare_lines, clear_byte
import json
pd.set_option('display.max_colwidth', None)
THRESHOLD = 200000
WRONG_GOLD_TABLES = ["bq095", "bq350", "bq379", "bq396", "sf_bq084", "sf_bq200","sf_bq226", "sf_bq295", "sf_bq358"]
SKIP_GOLD_SQLS = ["bq350", "local039", "bq095", "bq374", "bq379", "bq396", "bq403", "bq406", "sf_bq233", "sf_bq273", "sf_local039", "sf_bq295"]
def process_ddl(ddl_file):
    """
    DDL íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ í…Œì´ë¸” ëŒ€í‘œ ì´ë¦„ì„ ì°¾ê³ , ì¤‘ë³µëœ í…Œì´ë¸”ì„ ì œê±°í•©ë‹ˆë‹¤.

    Args:
        ddl_file (pd.DataFrame): DDL íŒŒì¼ì„ ì½ì–´ì˜¨ ë°ì´í„°í”„ë ˆì„

    Returns:
        ddl_file (pd.DataFrame): ì¤‘ë³µëœ í…Œì´ë¸”ì„ ì œê±°í•œ DDL íŒŒì¼
        representatives (dict): ê·¸ë£¹ë³„ ëŒ€í‘œ í…Œì´ë¸”ë“¤ì„ ì €ì¥í•œ ë”•ì…”ë„ˆë¦¬
            - Example:
                representatives = {
                    'users_': ['users_2020', 'users_2021', 'users_2022', ..., 'users_2023'],  # 15ê°œ
                    'orders_': ['orders_2020', 'orders_2021', 'orders_2022'],  # 3ê°œ
                    'products_': ['products_main']  # 1ê°œ
                }
    """
    table_names = ddl_file['table_name'].to_list() # e.g., ['users_2020', 'users_2021', 'orders_2020', 'orders_2021']
    representatives = {} # ê·¸ë£¹ë³„ ëŒ€í‘œ í…Œì´ë¸”ë“¤ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬

    # 1. ìˆ«ì ì œê±° í›„ ê·¸ë£¹í™”
    for i in range(len(ddl_file)):
        if remove_digits(table_names[i]) in representatives.keys(): 
            representatives[remove_digits(table_names[i])] += [table_names[i]]
        else:
            representatives[remove_digits(table_names[i])] = [table_names[i]]

    # 2. ì¤‘ë³µëœ í…Œì´ë¸” ì œê±°
    for i in range(len(ddl_file)):
        if remove_digits(table_names[i]) in representatives:
            if len(representatives[remove_digits(table_names[i])]) > 10: # 10ê°œ ì´ìƒì˜ í…Œì´ë¸”ì´ ìˆìœ¼ë©´ ëŒ€í‘œ í…Œì´ë¸”ë¡œ ì„ ì •
                if ddl_file['table_name'][i] != representatives[remove_digits(table_names[i])][0]:
                    ddl_file = ddl_file.drop(index=i)
            else:
                # representatives[table_names[i]] = [table_names[i]]
                del representatives[remove_digits(table_names[i])] # 10ê°œ ë¯¸ë§Œì˜ í…Œì´ë¸”ì´ ìˆìœ¼ë©´ ê·¸ë£¹ ì œê±°
    return ddl_file, representatives

def process_ddl_gold(ddl_file, gold_table_names, entry=None):
    table_names = ddl_file['table_name'].to_list()
    representatives = {}

    for i in range(len(ddl_file)):
        if not any(table_names[i].upper() in t for t in gold_table_names):
            ddl_file = ddl_file.drop(index=i)
    ddl_file.reset_index(drop=True, inplace=True)
    table_names = ddl_file['table_name'].to_list()

    for i in range(len(ddl_file)):
        if remove_digits(table_names[i]) in representatives.keys():
            representatives[remove_digits(table_names[i])] += [table_names[i]]
        else:
            representatives[remove_digits(table_names[i])] = [table_names[i]]

    for i in range(len(ddl_file)):
        if remove_digits(table_names[i]) in representatives:
            if len(representatives[remove_digits(table_names[i])]) > 10:
                if ddl_file['table_name'][i] != representatives[remove_digits(table_names[i])][0]:
                    ddl_file = ddl_file.drop(index=i)
            else:
                # representatives[table_names[i]] = [table_names[i]]
                del representatives[remove_digits(table_names[i])]

    return ddl_file, representatives

def process_ddl_gold_schema(ddl_file, full_table_names_with_omit, entry):
    table_names = ddl_file['table_name'].to_list()
    representatives = {}

    for i in range(len(ddl_file)):
        if not any(table_names[i].upper() in t for t in full_table_names_with_omit):
            if not any(remove_digits(table_names[i].upper()) in t for t in full_table_names_with_omit):
                ddl_file = ddl_file.drop(index=i)
    ddl_file.reset_index(drop=True, inplace=True)
    table_names = ddl_file['table_name'].to_list()

    for i in range(len(ddl_file)):
        if remove_digits(table_names[i]) in representatives.keys():
            representatives[remove_digits(table_names[i])] += [table_names[i]]
        else:
            representatives[remove_digits(table_names[i])] = [table_names[i]]

    for i in range(len(ddl_file)):
        if remove_digits(table_names[i]) in representatives:
            if len(representatives[remove_digits(table_names[i])]) > 10:
                if ddl_file['table_name'][i] != representatives[remove_digits(table_names[i])][0]:
                    ddl_file = ddl_file.drop(index=i)
            else:
                # representatives[table_names[i]] = [table_names[i]]
                del representatives[remove_digits(table_names[i])]

    return ddl_file, representatives

def check_table_names(ddl_path):
    ddl_file = pd.read_csv(ddl_path)
    temp_path = ddl_path.replace("DDL.csv", "DDL_tmp.csv")
    ddl_file['table_name'] = ddl_file['table_name'].str.split('.').str[-1]
    ddl_file.to_csv(temp_path, index=False)
    os.replace(temp_path, ddl_path)

# Step 1: (Optional) Raw dataì˜ ë³µì¡í•œ í´ë” êµ¬ì¡°ë¥¼ ì •ë¦¬
def make_folder(args):
    '''
    Raw dataì˜ ë³µì¡í•œ í´ë” êµ¬ì¡°ë¥¼ ì •ë¦¬

    - Before: examples_snow/sf_bq070/IDC/bigquery-public-data.idc_v18.dicom_all.json
                            â†“ Reconstruct â†“
    - After: examples_snow/sf_bq070/IDC/bigquery-public-data/idc_v18/dicom_all.json
    '''

    print("=" * 60)
    print("ğŸ“ STEP: í´ë” êµ¬ì¡° ì •ë¦¬ ì‹œì‘")
    print("=" * 60)
    
    example_folder = args.example_folder
    entries = os.listdir(example_folder)
    print(f"ğŸ“‚ ëŒ€ìƒ í´ë”: {example_folder}")
    print(f"ğŸ“‹ ì²˜ë¦¬í•  ì˜ˆì œ ìˆ˜: {len(entries)}ê°œ")
    print("ğŸ”„ ë³µì¡í•œ íŒŒì¼ëª…ì„ ì²´ê³„ì ì¸ í´ë” êµ¬ì¡°ë¡œ ë³€í™˜ ì¤‘...")
    print()
    
    for i, entry in enumerate(tqdm(entries, desc="í´ë” êµ¬ì¡° ì •ë¦¬"), 1):
        entry1_path = os.path.join(example_folder, entry)
        if os.path.isdir(entry1_path):
            for project_name in os.listdir(entry1_path):
                project_name_path = os.path.join(entry1_path, project_name)
                if os.path.isdir(project_name_path):
                    for db_name in os.listdir(project_name_path):
                        db_name_path = os.path.join(project_name_path, db_name)
                        if db_name == "json":
                            os.remove(os.path.join(project_name_path, "json"))
                        elif (entry.startswith("sf") and db_name.endswith(".json")) or (entry.startswith("bq")) or (entry.startswith("ga")):
                            assert '.' in db_name.strip(".json")
                            folder_name = db_name.split(".")[0]
                            file_name = '.'.join(db_name.split(".")[1:])
                            folder_path = os.path.join(project_name_path, folder_name)
                            if not os.path.exists(folder_path):
                                os.mkdir(folder_path)
                            if entry.startswith("bq") or entry.startswith("ga"):
                                shutil.move(db_name_path, os.path.join(folder_path, file_name))
                            elif entry.startswith("sf"):
                                shutil.copy(db_name_path, os.path.join(folder_path, file_name))
                                os.remove(db_name_path)                                
                    if entry.startswith("sf") and "DDL.csv" in os.listdir(project_name_path):
                        ddl_path = os.path.join(project_name_path, "DDL.csv")
                        shutil.copy(ddl_path, os.path.join(folder_path, "DDL.csv"))
                        os.remove(ddl_path)
                    if entry.startswith("bq") or entry.startswith("ga"):
                        shutil.move(folder_path, os.path.join(entry1_path, folder_name))
                        shutil.rmtree(project_name_path)
    
    print("âœ… í´ë” êµ¬ì¡° ì •ë¦¬ ì™„ë£Œ")
    print()

def compress_ddl(example_folder, add_description=False, add_sample_rows=False, rm_digits=False, schema_linked=False, clear_long_eg_des=False, sqlite_sl_path=None, reduce_col=False, use_gold_table=False, use_gold_schema=False):
    """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì™€ ë©”íƒ€ë°ì´í„°ë¥¼ LLMì´ ì½ì„ ìˆ˜ ìˆëŠ” í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    ì´ í•¨ìˆ˜ëŠ” Spider2 ë°ì´í„°ì…‹ ì˜ˆì œë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´, í…Œì´ë¸” ë©”íƒ€ë°ì´í„°,
    ê·¸ë¦¬ê³  ì™¸ë¶€ ì§€ì‹ ë¬¸ì„œë“¤ì„ ì¶”ì¶œí•œ í›„ ì´ë¥¼ ê²°í•©í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì´ SQL ìƒì„± ì‘ì—…ì— ì‚¬ìš©í•  ìˆ˜ 
    ìˆëŠ” êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        example_folder (str): ì˜ˆì œ ì¸ìŠ¤í„´ìŠ¤ë“¤ì´ í¬í•¨ëœ í´ë” ê²½ë¡œ 
            (ì˜ˆ: "examples_snow").
        add_description (bool, optional): ì¶œë ¥ í”„ë¡¬í”„íŠ¸ì— ì»¬ëŸ¼ ì„¤ëª…ì„ í¬í•¨í• ì§€ ì—¬ë¶€. 
            ê¸°ë³¸ê°’ì€ False.
        add_sample_rows (bool, optional): ê° í…Œì´ë¸”ì˜ ìƒ˜í”Œ ë°ì´í„° í–‰ì„ í¬í•¨í• ì§€ ì—¬ë¶€. 
            ê¸°ë³¸ê°’ì€ False.
        rm_digits (bool, optional): í…Œì´ë¸”ëª…ì—ì„œ ìˆ«ìë¥¼ ì œê±°í•˜ê³  ìœ ì‚¬í•œ í…Œì´ë¸”ë“¤ì„ 
            ê·¸ë£¹í™”í• ì§€ ì—¬ë¶€ (ì˜ˆ: users_2021, users_2022 -> users). ê¸°ë³¸ê°’ì€ False.
        schema_linked (bool, optional): ìŠ¤í‚¤ë§ˆ ë§í¬ëœ DDL íŒŒì¼ì„ ì‚¬ìš©í• ì§€ ì—¬ë¶€ 
            (DDL.csv ëŒ€ì‹  DDL_sl.csv). ê¸°ë³¸ê°’ì€ False.
        clear_long_eg_des (bool, optional): í”„ë¡¬í”„íŠ¸ê°€ í¬ê¸° ì„ê³„ê°’ì„ ì´ˆê³¼í•  ë•Œ 
            ê¸´ ì„¤ëª…ì„ ì œê±°í• ì§€ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ False.
        sqlite_sl_path (str, optional): ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ìš© SQLite ìŠ¤í‚¤ë§ˆ ë§í‚¹ 
            ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ. ê¸°ë³¸ê°’ì€ None.
        reduce_col (bool, optional): ìŠ¤í‚¤ë§ˆ ë§í‚¹ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì»¬ëŸ¼ì„ ì¤„ì¼ì§€ ì—¬ë¶€. 
            schema_linked=Trueê°€ í•„ìš”í•¨. ê¸°ë³¸ê°’ì€ False.
        use_gold_table (bool, optional): ê³¨ë“œ í‘œì¤€ í…Œì´ë¸”ëª…ì„ ì‚¬ìš©í•´ í…Œì´ë¸”ì„ 
            í•„í„°ë§í• ì§€ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ False.
        use_gold_schema (bool, optional): ê³¨ë“œ í‘œì¤€ SQL ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•´ í…Œì´ë¸”ê³¼ 
            ì»¬ëŸ¼ì„ í•„í„°ë§í• ì§€ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ False.
    
    Returns:
        None: ì´ í•¨ìˆ˜ëŠ” ê° ì˜ˆì œ í´ë”ì— ì²˜ë¦¬ëœ í”„ë¡¬í”„íŠ¸ë¥¼ "prompts.txt" íŒŒì¼ë¡œ 
        ì €ì¥í•˜ì§€ë§Œ ê°’ì„ ë°˜í™˜í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.
    
    Raises(ì˜ˆì™¸):
        FileNotFoundError: í•„ìš”í•œ DDL.csv ë˜ëŠ” JSON ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°.
        json.JSONDecodeError: JSON ë©”íƒ€ë°ì´í„° íŒŒì¼ í˜•ì‹ì´ ì˜ëª»ëœ ê²½ìš°.
        pandas.errors.EmptyDataError: DDL.csv íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì†ìƒëœ ê²½ìš°.
    
    Note:
        - ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ëŠ” ê° ì˜ˆì œ ë””ë ‰í† ë¦¬ì— "prompts.txt"ë¡œ ì €ì¥ë©ë‹ˆë‹¤
        - 200KB ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” í”„ë¡¬í”„íŠ¸ëŠ” ìë™ìœ¼ë¡œ ì••ì¶•ë©ë‹ˆë‹¤
        - .md íŒŒì¼ì˜ ì™¸ë¶€ ì§€ì‹ì´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤
        - ë¡œì»¬ SQLite ë°ì´í„°ë² ì´ìŠ¤ì˜ ê²½ìš° get_sqlite_data() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
        - í…Œì´ë¸” êµ¬ì¡° ì •ë³´ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í¬ë§·ë©ë‹ˆë‹¤: 
          {ë°ì´í„°ë² ì´ìŠ¤ëª…: {ìŠ¤í‚¤ë§ˆëª…: [í…Œì´ë¸”ëª…ë“¤]}}
    
    Example:
        >>> compress_ddl(
        ...     example_folder="examples_snow",
        ...     add_description=True,
        ...     add_sample_rows=True,
        ...     rm_digits=True,
        ...     clear_long_eg_des=True
        ... )
        # examples_snow/ ë‚´ ëª¨ë“  ì˜ˆì œë¥¼ ì²˜ë¦¬í•˜ê³  prompts.txt íŒŒì¼ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤
    """
    print("=" * 60)
    print("ğŸ—„ï¸  STEP: DDL ì••ì¶• ë° í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œì‘")
    print("=" * 60)
    
    entries = os.listdir(example_folder)
    print(f"ğŸ“‚ ëŒ€ìƒ í´ë”: {example_folder}")
    print(f"ğŸ“‹ ì²˜ë¦¬í•  ì˜ˆì œ ìˆ˜: {len(entries)}ê°œ")
    print(f"âš™ï¸  ì˜µì…˜ ì„¤ì •:")
    print(f"   - ì„¤ëª… ì¶”ê°€: {'âœ…' if add_description else 'âŒ'}")
    print(f"   - ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€: {'âœ…' if add_sample_rows else 'âŒ'}")
    print(f"   - ìˆ«ì ì œê±°: {'âœ…' if rm_digits else 'âŒ'}")
    print(f"   - ê¸´ ì„¤ëª… ì œê±°: {'âœ…' if clear_long_eg_des else 'âŒ'}")
    print(f"   - ê³¨ë“œ í…Œì´ë¸” ì‚¬ìš©: {'âœ…' if use_gold_table else 'âŒ'}")
    print(f"   - ê³¨ë“œ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©: {'âœ…' if use_gold_schema else 'âŒ'}")
    print()
    
    # 1. ê° example í´ë” ìˆœíšŒ
    for entry_idx, entry in enumerate(tqdm(entries, desc="ì˜ˆì œ ì²˜ë¦¬"), 1): 
        external_knowledge = None
        prompts = ''
        entry1_path = os.path.join(example_folder, entry)
        if os.path.isdir(entry1_path):
            print(f"  [{entry_idx:3d}/{len(entries)}] ğŸ”„ {entry} ì²˜ë¦¬ ì¤‘...")
            
            # 2. ê° ì˜ˆì œë³„ ê³¨ë“œ ë°ì´í„° ì²˜ë¦¬
            gold_table_names = None
            gold_column_names = None

            # 2-1. ê³¨ë“œ í…Œì´ë¸” ì‚¬ìš© ì‹œ 
            if use_gold_table:
                for ex in gold:
                    if ex['instance_id'] == entry:
                        gold_table_names = set([i.upper() for i in ex["gold_tables"]])
                if gold_table_names is None or entry in WRONG_GOLD_TABLES:
                    shutil.rmtree(os.path.join(args.example_folder, entry))
                    print(f"      âš ï¸  ê³¨ë“œ í…Œì´ë¸” ëˆ„ë½ìœ¼ë¡œ ìŠ¤í‚µ: {entry}")
                    continue
                
            # 2-2. ê³¨ë“œ ìŠ¤í‚¤ë§ˆ ì‚¬ìš© ì‹œ
            elif use_gold_schema:
                if entry in SKIP_GOLD_SQLS:
                    shutil.rmtree(os.path.join(args.example_folder, entry))
                    continue
                for ex in os.listdir(args.gold_sql_pth):
                    if ex.replace(".sql", "") == entry:
                        with open(os.path.join(args.gold_sql_pth, ex)) as f:
                            gold_sql = remove_declare_lines(f.read())
                        full_table_names_with_omit, gold_column_names = extract_real_table_names(gold_sql, get_api_name(ex))
                        gold_table_names = clear_name(full_table_names_with_omit, do_remove_digits=False)
                        gold_column_names = {i.upper() for i in gold_column_names}
                if gold_table_names is None:
                    shutil.rmtree(os.path.join(args.example_folder, entry))
                    print(f"      âš ï¸  ê³¨ë“œ ìŠ¤í‚¤ë§ˆ ëˆ„ë½ìœ¼ë¡œ ìŠ¤í‚µ: {entry}")
                    continue
                print(f"      âœ… ê³¨ë“œ ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì™„ë£Œ: {len(gold_table_names)}ê°œ í…Œì´ë¸”")

            # 3. DB ìŠ¤í‚¤ë§ˆ ì²˜ë¦¬
            # 3-1. ë¡œì»¬ DBê°€ ì•„ë‹Œ ê²½ìš° (Snowflake, BigQuery, Google BigQuery)
            if not entry.startswith("local"):
                print(f"      ğŸ—„ï¸  í´ë¼ìš°ë“œ DB ìŠ¤í‚¤ë§ˆ ì²˜ë¦¬ ì‹œì‘...")
                table_dict = {}
                project_count = 0
                table_count = 0

                # í”„ë¡œì íŠ¸ â†’ ë°ì´í„°ë² ì´ìŠ¤ â†’ ìŠ¤í‚¤ë§ˆ ìˆœìœ¼ë¡œ ìˆœíšŒ
                # í”„ë¡œì íŠ¸ ìˆœíšŒ
                for project_name in os.listdir(entry1_path):
                    if project_name == "spider":
                        continue
                    project_name_path = os.path.join(entry1_path, project_name)
                    if os.path.isdir(os.path.join(project_name_path)):
                        project_count += 1
                        print(f"        ğŸ“Š í”„ë¡œì íŠ¸: {project_name}")
                        # ë°ì´í„°ë² ì´ìŠ¤ ìˆœíšŒ
                        for db_name in os.listdir(project_name_path):
                            db_name_path = os.path.join(project_name_path, db_name)
                            assert os.path.isdir(db_name_path) == True and "DDL.csv" in os.listdir(db_name_path)

                            # ìŠ¤í‚¤ë§ˆ ìˆœíšŒ
                            for schema_name in os.listdir(db_name_path):
                                schema_name_path = os.path.join(db_name_path, schema_name)

                                # ìŠ¤í‚¤ë§ˆ íŒŒì¼ ì²˜ë¦¬
                                if schema_name == "DDL.csv":
                                    representatives = None

                                    if entry.startswith("sf0"): # "sf0"ê°€ ë­˜ê¹Œ? 
                                        check_table_names(schema_name_path)

                                    # ìŠ¤í‚¤ë§ˆ ë§í¬ê°€ ì°¸ì¸ ê²½ìš°, DDL ëŒ€ì‹  DDL_sl.csv íŒŒì¼ ì‚¬ìš©
                                    ddl_sl_flag = False
                                    if schema_linked:
                                        if os.path.exists(schema_name_path.replace("DDL.csv", "DDL_sl.csv")):
                                            ddl_sl_flag = True
                                            schema_name_path = schema_name_path.replace("DDL.csv", "DDL_sl.csv")
                                    ddl_file = pd.read_csv(schema_name_path)
                                    
                                    # clear ddl_file for sf
                                    # if entry.startswith("sf"):
                                    #     table_names_ = []
                                    #     for i in os.listdir(db_name_path):
                                    #         if i.endswith(".json"):
                                    #             table_names_ += [i.replace(".json", "").split(".")[-1]]
                                    #     ddl_file = ddl_file[ddl_file["table_name"].isin(table_names_)].reset_index(drop=True)
                                    #     assert not ddl_file.empty
                                    #     ddl_file.to_csv(schema_name_path, index=False)
                                    # print(ddl_file, entry)

                                    # ì¡°ê±´ì— ë”°ë¥¸ DDL ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
                                    if schema_linked and len(ddl_file['table_name'].to_list()) < 10:
                                        pass
                                    elif use_gold_table:
                                        ddl_file, representatives = process_ddl_gold(ddl_file, gold_table_names, entry)
                                    elif use_gold_schema:
                                        ddl_file, representatives = process_ddl_gold_schema(ddl_file, gold_table_names, entry)
                                    elif rm_digits:
                                        ddl_file, representatives = process_ddl(ddl_file)
                                    table_name_list = ddl_file['table_name'].to_list()
                                    ddl_file.reset_index(drop=True, inplace=True)
                                    print(f"          ğŸ—‚ï¸  ë°ì´í„°ë² ì´ìŠ¤: {db_name} ({len(table_name_list)}ê°œ í…Œì´ë¸”)")

                                    # 3-1-1. í…Œì´ë¸”ë³„ ìƒì„¸ ì •ë³´ ì²˜ë¦¬
                                    for i in range(len(table_name_list)):
                                        # 3-1-1-1. í…Œì´ë¸” JSON ë©”íƒ€ë°ì´í„° ë¡œë“œ
                                        if os.path.exists(os.path.join(db_name_path, table_name_list[i]+".json")):           
                                            with open(os.path.join(db_name_path, table_name_list[i]+".json"), encoding="utf-8") as f:
                                                table_json = json.load(f)
                                        elif os.path.exists(os.path.join(db_name_path, db_name+'.'+table_name_list[i]+".json")):
                                            with open(os.path.join(db_name_path, db_name+'.'+table_name_list[i]+".json"), encoding="utf-8") as f:
                                                table_json = json.load(f)
                                        else:
                                            print(f"            âš ï¸  í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ëˆ„ë½: {table_name_list[i]}")
                                            continue

                                        table_count += 1
                                        # 3-1-1-2. ê³¨ë“œ í…Œì´ë¸” ì‚¬ìš© ì‹œ
                                        if use_gold_table:
                                            if table_json["table_fullname"].upper() not in gold_table_names and not representatives:
                                                continue
                                        elif use_gold_schema:
                                            if table_json["table_fullname"].upper() not in gold_table_names and not representatives:
                                                continue
                                        
                                        # 3-1-1-3. í…Œì´ë¸” ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
                                        prompts += "í…Œì´ë¸” ì „ì²´ëª…: " + table_json["table_fullname"] + "\n"
                                        
                                        project_name_, db_name_, table_name_ = table_json["table_fullname"].split(".")
                                        table_dict.setdefault(project_name_, {}).setdefault(db_name_, [])


                                        if reduce_col and ddl_sl_flag:
                                            assert schema_linked
                                            full_name = table_json["table_fullname"]
                                            short_name = full_name.split(".")[-1].strip()

                                            ddl_file.columns = ddl_file.columns.str.strip().str.lower()
                                            ddl_file["table_name"] = ddl_file["table_name"].str.strip()
                                            matched = ddl_file[ddl_file["table_name"] == short_name].iloc[0]
                                            # assert len(matched) == 1, print(ddl_file["table_name"], short_name, entry)
                                            
                                            col_names = matched["ddl"]
                                        column_prefix = "column_"

                                        # 3-1-1-4. ì»¬ëŸ¼ ì •ë³´ ì¶”ê°€
                                        for j in range(len(table_json[f"{column_prefix}names"])):
                                            table_des = ''
                                            if add_description: # --add_description í”Œë˜ê·¸ ì‚¬ìš© ì‹œ
                                                if j < len(table_json["description"]):
                                                    table_des = " ì„¤ëª…: " + str(table_json["description"][j]) if table_json["description"][j] else ""
                                                elif table_json[f"column_names"][j] != "_PARTITIONTIME":
                                                    print(f"{entry} ì„¤ëª… ë¶ˆì¼ì¹˜ {table_name_list[i]}")

                                            if reduce_col and ddl_sl_flag:
                                                if table_json[f"{column_prefix}names"][j] in col_names:
                                                    prompts += "ì»¬ëŸ¼ëª…: " + table_json[f"{column_prefix}names"][j] + " íƒ€ì…: " + table_json[f"{column_prefix}types"][j] + table_des +"\n"
                                            elif use_gold_schema:
                                                if table_json[f"{column_prefix}names"][j].upper() in gold_column_names:
                                                    prompts += "ì»¬ëŸ¼ëª…: " + table_json[f"{column_prefix}names"][j] + " íƒ€ì…: " + table_json[f"{column_prefix}types"][j] + table_des +"\n"
                                            else:
                                                prompts += "ì»¬ëŸ¼ëª…: " + table_json[f"{column_prefix}names"][j] + " íƒ€ì…: " + table_json[f"{column_prefix}types"][j] + table_des +"\n"
                                        
                                        # 3-1-1-5. ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€
                                        if add_sample_rows: # --add_sample_rows í”Œë˜ê·¸ ì‚¬ìš© ì‹œ
                                            if reduce_col and ddl_sl_flag:
                                                sample_rows = [{col: row[col] for col in extract_column_names(col_names) if col in row} for row in table_json["sample_rows"]]
                                            elif use_gold_schema:
                                                rows = []
                                                for row in table_json["sample_rows"]:
                                                    for col in gold_column_names:
                                                        for s in row.keys():
                                                            if col in s.upper():
                                                                rows.append({col: row[s]})
                                                if table_json["sample_rows"]:
                                                    assert rows, str(entry)+str(table_json) + str(gold_column_names)
                                                sample_rows = rows
                                            else:
                                                sample_rows = table_json["sample_rows"]
                                            sample_rows = clear_byte(sample_rows)
                                            prompts += "ìƒ˜í”Œ ë°ì´í„°:\n" + str(sample_rows) + "\n"
                                        table_dict[project_name_][db_name_] += [table_name_list[i]]
                                        if representatives is not None:
                                            if remove_digits(table_name_list[i]) in representatives:
                                                if len(representatives[remove_digits(table_name_list[i])]) > 1:
                                                    assert len(representatives[remove_digits(table_name_list[i])]) >= 10, representatives[remove_digits(table_name_list[i])]
                                                    prompts += f"ìœ ì‚¬í•œ êµ¬ì¡°ë¥¼ ê°€ì§„ ë‹¤ë¥¸ í…Œì´ë¸”ë“¤: {representatives[remove_digits(table_name_list[i])]}\n"
                                                    table_dict[project_name_][db_name_] += representatives[remove_digits(table_name_list[i])]
                                        prompts += "\n" + "-" * 50 + "\n"
                                elif schema_name == "json":
                                    with open(schema_name_path, encoding="utf-8") as f:
                                        prompts += f.read()
                                        print(f.read())

                    # 3-1-2. .md íŒŒì¼ì˜ ì™¸ë¶€ ì§€ì‹ì´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ í¬í•¨
                    elif is_file(project_name_path, "md"):
                        with open(project_name_path, encoding="utf-8") as f:
                            external_knowledge = f.read() # ì™¸ë¶€ ì§€ì‹ íŒŒì¼ ì½ê¸°
            
            # 3-2. ë¡œì»¬ DBì¸ ê²½ìš°
            else:
                print(f"      ğŸ’¾ ë¡œì»¬ SQLite DB ì²˜ë¦¬ ì‹œì‘...")
                # ë¡œì»¬ DB íŒŒì¼ ìˆœíšŒ
                for sqlite in os.listdir(entry1_path):
                    if sqlite.endswith(".sqlite"):
                        sqlite_path = os.path.join(entry1_path, sqlite)
                        print(f"        ğŸ“ SQLite íŒŒì¼: {sqlite}")
                if sqlite_sl_path:
                    with open(sqlite_sl_path, encoding="utf-8") as f:
                        sl_res = json.load(f)
                    for eg in sl_res:
                        if eg["instance_id"] == entry:
                            sl_info = eg
                            external_knowledge = "ê²€ìƒ‰ëœ ì»¬ëŸ¼ê³¼ ê°’ë“¤: " + str(sl_info['L_values']) if sl_info['L_values'] else ""
                table_names, prompts = get_sqlite_data(sqlite_path, entry, add_description=add_description, add_sample_rows=add_sample_rows, gold_table_names=gold_table_names, gold_column_names=gold_column_names)
            
            # 4. í”„ë¡¬í”„íŠ¸ ì €ì¥
            print(f"      ğŸ’¾ í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì €ì¥ ì¤‘...")
            original_size = len(prompts)
            
            with open(os.path.join(entry1_path, "prompts.txt"), "w", encoding="utf-8") as f:
                # 4-1. 200KB ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” í”„ë¡¬í”„íŠ¸ëŠ” ìë™ìœ¼ë¡œ ì••ì¶•
                if len(prompts) > THRESHOLD:
                    print(f"        ğŸ—œï¸  í”„ë¡¬í”„íŠ¸ í¬ê¸° ì´ˆê³¼ ({original_size:,} bytes > {THRESHOLD:,} bytes), ìƒ˜í”Œ ë°ì´í„° ì••ì¶• ì¤‘...")
                    prompts = clear_sample_rows(prompts, byte_limit=10000)
                    print(f"        âœ‚ï¸  ìƒ˜í”Œ ë°ì´í„° ì••ì¶• ì™„ë£Œ: {len(prompts):,} bytes")

                # 4-2. ê¸´ ì„¤ëª… ì œê±°
                if len(prompts) > THRESHOLD and clear_long_eg_des:
                    print(f"        ğŸ—œï¸  ì—¬ì „íˆ í¬ê¸° ì´ˆê³¼, ê¸´ ì„¤ëª… ì œê±° ì¤‘...")
                    prompts = clear_description(prompts)
                    print(f"        âœ‚ï¸  ì„¤ëª… ì œê±° ì™„ë£Œ: {len(prompts):,} bytes")

                # 4-3. ì™¸ë¶€ ì§€ì‹ ì¶”ê°€
                if external_knowledge:
                    print(f"        ğŸ“š ì™¸ë¶€ ì§€ì‹ ì¶”ê°€")
                prompts += f"ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ì™¸ë¶€ ì§€ì‹: \n{external_knowledge}\n"

                # 4-4. í…Œì´ë¸” êµ¬ì¡° ì •ë³´ ì¶”ê°€
                if not entry.startswith("local"):
                    prompts += "í…Œì´ë¸” êµ¬ì¡° ì •ë³´ ({ë°ì´í„°ë² ì´ìŠ¤ëª…: {ìŠ¤í‚¤ë§ˆëª…: [í…Œì´ë¸”ëª…]}}): \n" + str(table_dict) + "\n"
                    print(f"        ğŸ“Š í´ë¼ìš°ë“œ DB ì •ë³´ ì¶”ê°€: {project_count}ê°œ í”„ë¡œì íŠ¸, {table_count}ê°œ í…Œì´ë¸”")
                else:
                    prompts += "í…Œì´ë¸” êµ¬ì¡° ì •ë³´ (í…Œì´ë¸”ëª…ë“¤): \n" + str(table_names) + "\n"
                    print(f"        ğŸ’¾ SQLite DB ì •ë³´ ì¶”ê°€: {len(table_names)}ê°œ í…Œì´ë¸”")
                
                # 4-5. ìµœì¢… í”„ë¡¬í”„íŠ¸ ì €ì¥
                f.writelines(prompts)
                final_size = len(prompts)
                print(f"      âœ… í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ: {final_size:,} bytes")

# ì“¸ ì¼ ì—†ì„ ê²ƒìœ¼ë¡œ ë³´ì„
def get_sqlite_data(path, entry, add_description=False, add_sample_rows=False, gold_table_names=None, gold_column_names=None):
    connection = sqlite3.connect(path)
    cursor = connection.cursor()
    cursor.execute("SELECT name, sql FROM sqlite_master WHERE type='table'")
    tables = cursor.fetchall()

    table_names = [table[0] for table in tables]
    prompts = ""
    for table in tables:
        table_name = table[0]

        if gold_table_names:
            if table_name.upper() not in gold_table_names:
                continue

        table_json = {}
        table_json["table_fullname"] = table_name

        cursor.execute("PRAGMA table_info({})".format(table_name))
        columns_info = cursor.fetchall()
        column_names = []
        column_types = []
        for col in columns_info:
            column_names.append(col[1])
            column_types.append(col[2])

        if gold_column_names:
            table_json["column_names"] = []
            table_json["column_types"] = []
            for i in range(len(column_names)):
                if column_names[i].upper() in gold_column_names:
                    table_json["column_names"].append(column_names[i])
                    table_json["column_types"].append(column_types[i])
        else:
            table_json["column_names"] = column_names
            table_json["column_types"] = column_types

        if not table_json["column_names"]:
            print(f"          âš ï¸  ì»¬ëŸ¼ ë¶ˆì¼ì¹˜: {table_name}")

        sample_rows = []
        if add_sample_rows:
            if gold_column_names:
                column_str = ", ".join(table_json["column_names"])
                query = f"SELECT {column_str} FROM {table_name} LIMIT 3"
            else:
                query = f"SELECT * FROM {table_name} LIMIT 3"
            # print(query)
            cursor.execute(query)
            sample_rows = cursor.fetchall()
        table_json["sample_rows"] = str(sample_rows)

        prompts += "\n" + "-" * 50 + "\n"
        prompts += "í…Œì´ë¸” ì „ì²´ëª…: " + table_json["table_fullname"] + "\n"
        for j in range(len(table_json["column_names"])):
            table_des = ''
            prompts += "ì»¬ëŸ¼ëª…: " + table_json["column_names"][j] + " íƒ€ì…: " + table_json["column_types"][j] + table_des + "\n"
        if add_sample_rows:
            prompts += "ìƒ˜í”Œ ë°ì´í„°:\n" + table_json["sample_rows"] + "\n"
    connection.close()
    return table_names, prompts


if __name__ == '__main__':
    """
    type: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê°’ì„ ì–´ë–¤ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜í• ì§€
    action = "store_true": ì¸ìê°€ ìˆìœ¼ë©´ `True`, ì—†ìœ¼ë©´ `False`
    """
    parser = argparse.ArgumentParser()
    
    # compress_ddl í•¨ìˆ˜ì˜ ì¸ì
    parser.add_argument('--example_folder', type=str, default="examples")
    parser.add_argument('--add_description', action="store_true")
    parser.add_argument('--add_sample_rows', action="store_true")
    parser.add_argument('--rm_digits', action="store_true")
    parser.add_argument('--schema_linked', action="store_true")
    parser.add_argument('--clear_long_eg_des', action="store_true")
    parser.add_argument('--sqlite_sl_path', type=str, default=None)
    parser.add_argument('--reduce_col', action="store_true")
    parser.add_argument('--use_gold_table', action="store_true")
    parser.add_argument('--gold_table_pth', type=str, default=None) # ê³¨ë“œ í…Œì´ë¸” íŒŒì¼ ê²½ë¡œ
    parser.add_argument('--use_gold_schema', action="store_true")
    parser.add_argument('--gold_sql_pth', type=str, default=None) # ê³¨ë“œ ìŠ¤í‚¤ë§ˆ íŒŒì¼ ê²½ë¡œ

    # make_folder í•¨ìˆ˜ì˜ ì¸ì
    parser.add_argument('--make_folder', action="store_true")
    
    args = parser.parse_args()
    if args.make_folder: # í˜„ì¬: ì‚¬ìš© ì¤‘ O
        make_folder(args)

    # ê³¨ë“œ í…Œì´ë¸” ë¡œë“œ: ì •ë‹µ í…Œì´ë¸” ì •ë³´ë¥¼ ë‹´ì€ JSON íŒŒì¼ ë¡œë“œ
    if args.use_gold_table: # í˜„ì¬: ì‚¬ìš© ì¤‘ X
        gold_tb = args.gold_table_pth
        with open(gold_tb) as f:
            gold = [json.loads(i) for i in f]

    # ê³¨ë“œ ìŠ¤í‚¤ë§ˆ ë¡œë“œ: ì •ë‹µ SQL íŒŒì¼ë“¤ì´ ìˆëŠ” ê²½ë¡œ ë¡œë“œ
    elif args.use_gold_schema: # í˜„ì¬: ì‚¬ìš© ì¤‘ X
        gold_sql_pth = args.gold_sql_pth

    # compress_ddl í•¨ìˆ˜ í˜¸ì¶œ (Main)
    print("=" * 60)
    print("ğŸ‰ DDL ì••ì¶• ë° í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ!")
    print("=" * 60)
    
    compress_ddl(args.example_folder, 
                 args.add_description, args.add_sample_rows, 
                 args.rm_digits, args.schema_linked, args.clear_long_eg_des, 
                 args.sqlite_sl_path, args.reduce_col, 
                 args.use_gold_table, args.use_gold_schema)
    
    print("âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ğŸ“ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ íŒŒì¼ë“¤: {args.example_folder}/*/prompts.txt")
    print("ğŸ”§ ë‹¤ìŒ ë‹¨ê³„: schema_linking.py ì‹¤í–‰")